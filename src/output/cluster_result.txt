[
    {
        "cluster": "6",
        "lines": [
            "terry terry 2011-04-12 15:37:56",
            "转自 >> iteye-mubawa",
            "人工智能研究的方向之一，是以所谓 “专家系统” 为代表的，用大量 “如果-就” (if - then) 规则定义的，自上而下的思路。人工神经网络 ( artifical neural network)，标志着另外一种自下而上的思路。神经网络没有一个严格的正式定义。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。[4]",
            "a). autoencoder",
            "最简单的一种方法是利用人工神经网络的特点，人工神经网络（ann）本身就是具有层次结构的系统，如果给定一个神经网络，我们假设其输出与输入是相同的，然后训练调整其参数，得到每一层中的权重，自然地，我们就得到了输入i的几种不同表示（每一层代表一种表示），这些表示就是特征，在研究中可以发现，如果在原有的特征中加入这些自动学习得到的特征可以大大提高精确度，甚至在分类问题中比目前最好的分类算法效果还要好！这种方法称为autoencoder。当然，我们还可以继续加上一些约束条件得到新的deep learning方法，如如果在autoencoder的基础上加上l1的regularity限制（l1主要是约束每一层中的节点中大部分都要为0，只有少数不为0，这就是sparse名字的来源），我们就可以得到sparse autoencoder方法。",
            "通过求解这个最优化式子，我们可以求得系数wi和基bi，这些系数和基就是输入的另外一种近似表达，因此，可以用它们来特征表达输入i，这个过程也是自动学习得到的。如果我们在上述式子上加上l1的regularity限制，得到：",
            "假设有一个二部图，每一层的节点之间没有链接，一层是可视层，即输入数据层（v)，一层是隐藏层(h)，如果假设所有的节点都是二值变量节点（只能取0或者1值），同时假设全概率分布p(v, h)满足boltzmann 分布，我们称这个模型是restrict boltzmann machine (rbm)。下面我们来看看为什么它是deep learning方法。首先，这个模型因为是二部图，所以在已知v的情况下，所有的隐藏节点之间是条件独立的，即p(h|v) =p(h1|v).....p(hn|v)。同理，在已知隐藏层h的情况下，所有的可视节点都是条件独立的，同时又由于所有的v和h满足boltzmann 分布，因此，当输入v的时候，通过p(h|v) 可以得到隐藏层h，而得到隐藏层h之后，通过p(v|h) 又能得到可视层，通过调整参数，我们就是要使得从隐藏层得到的可视层v1与原来的可视层v如果一样，那么得到的隐藏层就是可视层另外一种表达，因此隐藏层可以作为可视层输入数据的特征，所以它就是一种deep learning方法。",
            "如果，我们把隐藏层的层数增加，我们可以得到deep boltzmann machine (dbm)；如果我们在靠近可视层的部分使用贝叶斯信念网络（即有向图模型，当然这里依然限制层中节点之间没有链接），而在最远离可视层的部分使用restrict boltzmann machine，我们可以得到deep belief net （dbn） 。",
            "当然，还有其它的一些deep learning 方法。总之，deep learning能够自动地学习出数据的另外一种表示方法，这种表示可以作为特征加入原有问题的特征集合中，从而可以提高学习方法的效果，是业界的研究热点。"
        ]
    },
    {
        "cluster": "8",
        "lines": [
            "用sed删除行首行尾的空格和tab",
            "在unix上使用sed命令进行字符串处理中常常遇到的问题就是行首行尾的空格怎么删除。",
            "1：行首空格",
            "中括号表示“或”，空格或tab中的任意一种。这是正则表达式的规范。",
            "2:行末空格",
            "但是要注意在ksh中，tab并不是\\t而是直接打入一个tab就可以了。"
        ]
    },
    {
        "cluster": "13",
        "lines": [
            "hinton, g. e., osindero, s. and teh, y.,a fast learning algorithm for deep belief nets.neural computation 18:1527-1554, 2006",
            "yoshua bengio, pascal lamblin, dan popovici and hugo larochelle,greedy layerwise training of deep networks, in j. platt et al. (eds), advances in neural information processing systems 19 (nips 2006), pp. 153-160, mit press, 2007",
            "marc’aurelio ranzato, christopher poultney, sumit chopra and yann lecun efficient learning of sparse representations with an energy-based model, in j. platt et al. (eds), advances in neural information processing systems (nips 2006), mit press, 2007",
            "imagenet classification with deep convolutional neural networks, alex krizhevsky, ilya sutskever, geoffrey e hinton, nips 2012.",
            "learning hierarchical features for scene labeling, clement farabet, camille couprie, laurent najman and yann lecun, ieee transactions on pattern analysis and machine intelligence, 2013.",
            "learning convolutional feature hierarchies for visual recognition, koray kavukcuoglu, pierre sermanet, y-lan boureau, karol gregor, micha&euml;l mathieu and yann lecun, advances in neural information processing systems (nips 2010), 23, 2010."
        ]
    },
    {
        "cluster": "20",
        "lines": [
            "从一个输入中产生一个输出所涉及的计算可以通过一个流向图(flow graph)来表示：流向图是一种能够表示计算的图，在这种图中每一个节点表示一个基本的计算并且一个计算",
            "的值(计算的结果被应用到这个节点的孩子节点的值)。考虑这样一个计算集合，它可以被允许在每一个节点和可能的图结构中，并定义了一个函数族。输入节点没有孩子，输出节点没有父亲。",
            "这种流向图的一个特别属性是深度(depth)：从一个输入到一个输出的最长路径的长度。",
            "传统的前馈神经网络能够被看做拥有等于层数的深度(比如对于输出层为隐层数加1)。svms有深度2(一个对应于核输出或者特征空间，另一个对应于所产生输出的线性混合)。[3]",
            "例如，视觉皮质得到了很好的研究，并显示出一系列的区域，在每一个这种区域中包含一个输入的表示和从一个到另一个的信号流(这里忽略了在一些层次并行路径上的关联，因此更复杂)。这个特征层次的每一层表示在一个不同的抽象层上的输入，并在层次的更上层有着更多的抽象特征，他们根据低层特征定义。",
            "需要注意的是大脑中的表示是在中间紧密分布并且纯局部：他们是稀疏的：1%的神经元是同时活动的。给定大量的神经元，仍然有一个非常高效地(指数级高效)表示。"
        ]
    },
    {
        "cluster": "2",
        "lines": [
            "c) restrict boltzmann machine (rbm)",
            "2、语音识别",
            "微软研究人员通过与hintion合作，首先将rbm和dbn引入到语音识别声学模型训练中，并且在大词汇量语音识别系统中获得巨大成功，使得语音识别的错误率相对减低30%。但是，dnn还没有有效的并行快速算法，很多研究机构都是在利用大规模数据语料通过gpu平台提高dnn声学模型的训练效率。",
            "在国际上，ibm、google等公司都快速进行了dnn语音识别的研究，并且速度飞快。",
            "国内方面，阿里巴巴，科大讯飞、百度、中科院自动化所等公司或研究单位，也在进行深度学习在语音识别上的研究。"
        ]
    },
    {
        "cluster": "4",
        "lines": [
            "研究者在使用聚类分析时应特别注意可能影响结果的各个因素。",
            "异常值和特殊的变量对聚类有较大影响",
            "不会自动给出一个最佳聚类结果；",
            "聚类算法要比距离测量方法对聚类结果影响更大；",
            "一般聚类个数在4－6类，不易太多，或太少；"
        ]
    },
    {
        "cluster": "7",
        "lines": [
            "第二个和第三个\\中间没有东西，表示空",
            "人脑具有一个深度结构。",
            "在许多情形中深度2就足够表示任何一个带有给定目标精度的函数。但是其代价是：图中所需要的节点数(比如计算和参数数量)可能变的非常大。理论结果证实那些事实上所需要的节点数随着输入的大小指数增长的函数族是存在的。",
            "我们可以将深度架构看做一种因子分解。大部分随机选择的函数不能被有效地表示，无论是用深的或者浅的架构。但是许多能够有效地被深度架构表示的却不能被用浅的架构高效表示。一个紧的和深度的表示的存在意味着在潜在的可被表示的函数中存在某种结构。如果不存在任何结构，那将不可能很好地泛化。",
            "大脑有一个深度架构"
        ]
    },
    {
        "cluster": "14",
        "lines": [
            "中括号右边是*，表示一个或多个。",
            "同机器学习方法一样，深度机器学习方法也有监督学习与无监督学习之分．不同的学习框架下建立的学习模型很是不同．例如，卷积神经网络（convolutional neural networks，简称cnns）就是一种深度的监督学习下的机器学习模型，而深度置信网（deep belief nets，简称dbns）就是一种无监督学习下的机器学习模型。",
            "含多个隐层的深度学习模型",
            "含多个隐层的深度学习模型",
            "工程师将任务分解成多个抽象层次去处理；"
        ]
    },
    {
        "cluster": "17",
        "lines": [
            "①无监督学习用于每一层网络的pre-train；",
            "②每次用无监督学习只训练一层，将其训练结果作为其高一层的输入；",
            "表示的无监督学习被用于(预)训练每一层；",
            "在一个时间里的一个层次的无监督训练，接着之前训练的层次。在每一层学习到的表示作为下一层的输入；",
            "dbns在每一层中利用用于表示的无监督学习rbms。bengio et al paper 探讨和对比了rbms和auto-encoders(通过一个表示的瓶颈内在层预测输入的神经网络)。ranzato et al paper在一个convolutional架构的上下文中使用稀疏auto-encoders(类似于稀疏编码)。auto-encoders和convolutional架构将在以后的课程中讲解。"
        ]
    },
    {
        "cluster": "21",
        "lines": [
            "我们也可以对变量进行聚类—分类，但是更常见的还是对个体分类（样本聚类——细分）。为了得到比较合理的分类，首先要采用适当的指标来定量地描述研究对象（样本或变量，常用的是样本）之间的联系的紧密程度。常用的指标为“距离”和“相似系数”，假定研究对象均用所谓的“点”来表示。",
            "需要一组表示个体性质或特征的变量，称之为聚类变量。根据个体或样本之间联系的紧密程度进行分类。一般来说分类变量的组合都是由研究者规定的，不是像其它多元分析方法那样估计推导出来的。",
            "根据聚类变量得到的描述两个个体间（或变量间）的对应程度或联系紧密程度的度量。",
            "采用描述个体对（变量对）之间的接近程度的指标，例如“距离”，“距离”越小的个体（变量）越具有相似性。",
            "采用表示相似程度的指标，例如“相关系数”，“相关系数”越大的个体（变量）越具有相似性。"
        ]
    },
    {
        "cluster": "23",
        "lines": [
            "话是这么说了，但还是沿用过去的方式来讲讲聚类分析cluster analysis吧！物以类聚，人以群分，聚类分析是一种重要的多变量统计方法，但记住其实它是一种数据分析方法，不能进行统计推断的。当然，聚类分析主要应用在市场细分等领域，我们也经常采用聚类分析技术来实现对抽样框的分层，我就不多罗嗦了。",
            "聚类分析：顾名思义是一种分类的多元统计分析方法。按照个体或样品(individuals, objects or subjects)的特征将它们分类，使同一类别内的个体具有尽可能高的同质性(homogeneity)，而类别之间则应具有尽可能高的异质性(heterogeneity)。",
            "聚类分析前所有个体或样本所属的类别是未知的，类别个数一般也是未知的，分析的依据就是原始数据，没有任何事先的有关类别的信息可参考。所以：严格说来聚类分析并不是纯粹的统计技术，它不像其它多元分析法那样，需要从样本去推断总体。聚类分析一般都涉及不到有关统计量的分布，也不需要进行显着性检验。聚类分析更像是一种建立假设的方法，而对假设的检验还需要借助其它统计方法。",
            "聚类分析主要应用于探索性的研究，其分析的结果可以提供多个可能的解，选择最终的解需要研究者的主观判断和后续的分析；",
            "不管实际数据中是否真正存在不同的类别，利用聚类分析都能得到分成若干类别的解；"
        ]
    },
    {
        "cluster": "25",
        "lines": [
            "聚类分析的解完全依赖于研究者所选择的聚类变量，增加或删除一些变量对最终的解都可能产生实质性的影响。",
            "当分类变量的测量尺度不一致时，需要事先做标准化处理。",
            "样本聚类，变量之间的关系需要研究者决定；",
            "聚类变量的测量尺度不同，需要事先对变量标准化；",
            "聚类变量中如果有些变量非常相关，意味着这个变量的权重会更大"
        ]
    },
    {
        "cluster": "5",
        "lines": [
            "深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。[1]",
            "深度学习的概念由hinton等人于2006年提出。基于深度置信网络(dbn)提出非监督贪心逐层训练算法，为解决深层结构相关的优化难题带来希望，随后提出多层自动编码器深层结构。此外lecun等人提出的卷积神经网络是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高训练性能。[1]",
            "需要使用深度学习解决的问题有以下的特征：",
            "人类首先学习简单的概念，然后用他们去表示更抽象的；"
        ]
    },
    {
        "cluster": "16",
        "lines": [
            "深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。[2]",
            "深度学习的核心思想",
            "深度学习的核心思想",
            "把学习结构看作一个网络，则深度学习的核心思路如下："
        ]
    },
    {
        "cluster": "9",
        "lines": [
            "③用自顶而下的监督算法去调整所有层",
            "2006年前，尝试训练深度架构都失败了：训练一个深度有监督前馈神经网络趋向于产生坏的结果(同时在训练和测试误差中)，然后将其变浅为1(1或者2个隐层)。",
            "用有监督训练来调整所有层(加上一个或者更多的用于产生预测的附加层)；"
        ]
    },
    {
        "cluster": "10",
        "lines": [
            "人类层次化地组织思想和概念；",
            "学习/发现这些概念(知识工程由于没有反省而失败？)是很美好的。对语言可表达的概念的反省也建议我们一个稀疏的表示：仅所有可能单词/概念中的一个小的部分是可被应用到一个特别的输入(一个视觉场景)。[3]",
            "1、计算机视觉"
        ]
    },
    {
        "cluster": "12",
        "lines": [
            "每周一讲，其实早不是每周了，可见人要是能够做一件事情容易，坚持做就太难了，以后别自己给自己定目标，随心情多好！",
            "聚类分析简单、直观。",
            "当然，聚类分析不能做的事情是："
        ]
    },
    {
        "cluster": "15",
        "lines": [
            "标准化方法影响聚类模式：",
            "变量标准化倾向产生基于数量的聚类；",
            "样本标准化倾向产生基于模式的聚类；"
        ]
    },
    {
        "cluster": "22",
        "lines": [
            "第一个/的左边是s表示替换，即将空格替换为空",
            "g表示替换原来buffer中的，sed在处理字符串的时候并不对源文件进行直接处理，先创建一个buffer，但是加g表示对原buffer进行替换",
            "整体的意思是：用空字符去替换一个或多个用空格或tab开头的本体字符串"
        ]
    },
    {
        "cluster": "29",
        "lines": [
            "如果我们把输出必须和输入相等的限制放松，同时利用线性代数中基的概念，即o = w1*b1 + w2*b2+....+ wn*bn， bi是基，wi是系数，我们可以得到这样一个优化问题：",
            "min |i - o|",
            "min |i - o| + u*(|w1| + |w2| + ... + |wn|)"
        ]
    },
    {
        "cluster": "30",
        "lines": [
            "在聚类分析中，一般的规则是将“距离”较小的点或“相似系数”较大的点归为同一类，将“距离”较大的点或“相似系数”较小的点归为不同的类！（一般的相似系数就是相关系数了）",
            "计算聚类——距离指标d(distance)的方法非常多：按照数据的不同性质，可选用不同的距离指标。欧氏距离(euclidean distance)、欧氏距离的平方(squared euclidean distance)、曼哈顿距离(block)、切比雪夫距离(chebychev distance)、卡方距离(chi-aquare measure) 等；相似性也有不少，主要是皮尔逊相关系数了！",
            "欧式距离的平方是最常用的距离测量方法；"
        ]
    },
    {
        "cluster": "32",
        "lines": [
            "下面介绍sed是怎样实现的，当然awk同样可以。",
            "sed 's/^[ \\t]*//g'",
            "sed 's/[ \\t]*$//g'"
        ]
    },
    {
        "cluster": "3",
        "lines": [
            "深度不足会出现问题。",
            "深度不足会出现问题"
        ]
    },
    {
        "cluster": "11",
        "lines": [
            "我这里提到的聚类分析主要是谱系聚类（hierarchical clustering）和快速聚类（k-means）、两阶段聚类（two-step）；",
            "注意：上面主要在谱系聚类方法中采用，但谱系聚类主要用在变量聚类上，如果对样本聚类样本不能太多了，否则你要等很长时间，还不一定有用！"
        ]
    },
    {
        "cluster": "18",
        "lines": [
            "认知过程逐层进行，逐步抽象。",
            "认知过程逐层进行，逐步抽象"
        ]
    },
    {
        "cluster": "19",
        "lines": [
            "b). sparse coding",
            "这种方法被称为sparse coding。"
        ]
    },
    {
        "cluster": "24",
        "lines": [
            "3、自然语言处理等其他领域",
            "很多机构在开展研究，2013年tomas mikolov,kai chen,greg corrado,jeffrey dean发表论文efficient estimation of word representations in vector space建立word2vector模型，与传统的词袋模型（bag of words）相比，word2vector能够更好地表达语法信息。[6]  深度学习在自然语言处理等领域主要应用于机器翻译以及语义挖掘等方面。"
        ]
    },
    {
        "cluster": "27",
        "lines": [
            "第一个/的右边是表示后面的以xx开头",
            "和上面稍微有些不同是前面删除了^符，在后面加上了美元符，这表示以xx结尾的字符串为对象。"
        ]
    },
    {
        "cluster": "28",
        "lines": [
            "自动发现和告诉你应该分成多少个类——属于非监督类分析方法",
            "在这三篇论文中以下主要原理被发现："
        ]
    },
    {
        "cluster": "31",
        "lines": [
            "2006年的3篇论文改变了这种状况，由hinton的革命性的在深度信念网(deep belief networks, dbns)上的工作所引领：",
            "从2006年以来，大量的关于深度学习的论文被发表。"
        ]
    },
    {
        "cluster": "1",
        "lines": [
            "可以用两种方式来测量："
        ]
    },
    {
        "cluster": "26",
        "lines": [
            "期望能很清楚的找到大致相等的类或细分市场是不现实的；"
        ]
    }
]